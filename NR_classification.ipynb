{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b105f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "import random as rd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import CustomDataset as cd\n",
    "import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e659cc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, batch_size = 50):\n",
    "    model.train()\n",
    "    #loop run over data output by loader to train model\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) #applies model to data\n",
    "        # print(target.shape)\n",
    "        target = target.squeeze(1) #removes dimension from target\n",
    "        # print(target.shape)\n",
    "        # print(output)\n",
    "        # print(target.shape)\n",
    "        # loss = nn.CrossEntropyLoss()(output, target[:,1].long()) #calculates cross entropy loss\n",
    "    \n",
    "        loss = nn.CrossEntropyLoss()(output, target.long())\n",
    "        # print(loss)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() #new step in optimisation of loss\n",
    "        #printing output of each batch for loss observation while model is training\n",
    "        if batch_idx % batch_size == 0:\n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{} ({}%)]\\tLoss: {}'.format(\n",
    "                    epoch, batch_idx * len(data), len(train_loader.dataset), \\\n",
    "                    round(100 * batch_idx / len(train_loader),0), round(loss.item(),5)\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f4202b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    #initialisting parameters and empty lists\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    probs_pred = []\n",
    "    labels_actual = []\n",
    "    labels_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target=target.squeeze(1)\n",
    "            # print(output.shape)\n",
    "            # test_loss += nn.CrossEntropyLoss()(output, target[:,1].long()).item()\n",
    "            # print(output)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target.long()).item()\n",
    "\n",
    "            # pred = output.max(1, keepdim=True)[1] #finds index of maximum in output vector as described below\n",
    "\n",
    "            pred = output.max(1, keepdim=False)[1]#torch.round(output)\n",
    "\n",
    "            # print(output)\n",
    "            # pred_onehot = F.one_hot(pred).squeeze(1) #converts into one-hot predicted label \n",
    "            # print(pred_onehot)\n",
    "            # print(test_loss)\n",
    "            # pred_bool = torch.eq(pred_onehot,target) #finds values where predicted equals target\n",
    "            pred_bool = torch.eq(pred,target)\n",
    "            # print(pred_bool)\n",
    "            # correct += int((pred_bool.sum().item())/2) #sums number of correct values\n",
    "            correct += pred_bool.sum().item()\n",
    "            #adds predicted labels, targets and probabilities to lists for later use\n",
    "            labels_actual.append(target.cpu().numpy())\n",
    "            probs_pred.append(output.cpu().numpy())\n",
    "            # labels_pred.append(pred_onehot.cpu().numpy())\n",
    "\n",
    "    #printing accuracy and loss after each epoch for analysis\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\n",
    "        '\\nTest set: Average loss: {}, Accuracy: {}/{} ({}%)\\n'.format(\n",
    "              round(test_loss,4), correct, len(test_loader.dataset), \\\n",
    "            round(100 * correct / len(test_loader.dataset),0)\n",
    "            )\n",
    "          )\n",
    "  \n",
    "    return(labels_actual,probs_pred,labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c0a117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/C_50keV.npy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13620/2176391138.py:27: RuntimeWarning: invalid value encountered in true_divide\n",
      "  C_dat = (C_dat-np.nanmean(C_dat,axis=0))/np.nanstd(C_dat,axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/F_50keV.npy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13620/2176391138.py:42: RuntimeWarning: invalid value encountered in true_divide\n",
      "  F_dat = (F_dat-np.nanmean(F_dat,axis=0))/np.nanstd(F_dat,axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data/C_55keV.npy...\n",
      "Loading data/F_55keV.npy...\n",
      "Loading data/C_60keV.npy...\n",
      "Loading data/F_60keV.npy...\n",
      "Loading data/C_65keV.npy...\n",
      "Loading data/F_65keV.npy...\n",
      "Loading data/C_70keV.npy...\n",
      "Loading data/F_70keV.npy...\n",
      "Loading data/C_75keV.npy...\n",
      "Loading data/F_75keV.npy...\n",
      "Loading data/C_80keV.npy...\n",
      "Loading data/F_80keV.npy...\n",
      "Loading data/C_85keV.npy...\n",
      "Loading data/F_85keV.npy...\n",
      "Loading data/C_90keV.npy...\n",
      "Loading data/F_90keV.npy...\n",
      "Loading data/C_95keV.npy...\n",
      "Loading data/F_95keV.npy...\n",
      "Loading data/C_100keV.npy...\n",
      "Loading data/F_100keV.npy...\n",
      "Loading data/C_105keV.npy...\n",
      "Loading data/F_105keV.npy...\n",
      "Loading data/C_110keV.npy...\n",
      "Loading data/F_110keV.npy...\n",
      "Loading data/C_115keV.npy...\n",
      "Loading data/F_115keV.npy...\n",
      "Loading data/C_120keV.npy...\n",
      "Loading data/F_120keV.npy...\n",
      "Loading data/C_125keV.npy...\n",
      "Loading data/F_125keV.npy...\n",
      "Loading data/C_130keV.npy...\n",
      "Loading data/F_130keV.npy...\n",
      "Loading data/C_135keV.npy...\n",
      "Loading data/F_135keV.npy...\n",
      "Loading data/C_140keV.npy...\n",
      "Loading data/F_140keV.npy...\n",
      "Loading data/C_145keV.npy...\n",
      "Loading data/F_145keV.npy...\n",
      "Loading data/C_150keV.npy...\n",
      "Loading data/F_150keV.npy...\n",
      "Loading data/C_155keV.npy...\n",
      "Loading data/F_155keV.npy...\n",
      "Loading data/C_160keV.npy...\n",
      "Loading data/F_160keV.npy...\n",
      "Loading data/C_165keV.npy...\n",
      "Loading data/F_165keV.npy...\n",
      "Loading data/C_170keV.npy...\n",
      "Loading data/F_170keV.npy...\n",
      "Loading data/C_175keV.npy...\n",
      "Loading data/F_175keV.npy...\n",
      "Loading data/C_180keV.npy...\n",
      "Loading data/F_180keV.npy...\n",
      "Loading data/C_185keV.npy...\n",
      "Loading data/F_185keV.npy...\n",
      "Loading data/C_190keV.npy...\n",
      "Loading data/F_190keV.npy...\n",
      "Loading data/C_195keV.npy...\n",
      "Loading data/F_195keV.npy...\n"
     ]
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "\"\"\"\n",
    "For a learning algorithm, it will be necesarry to have an input (X data) containing a number of $97\\times97$ matrices for each energy. \n",
    "The output will be a label, either Carbon or Flourine. The model will compare its output with the actual labels and adjust accordingly to minimise loss. \n",
    "\n",
    "In order to quantify each label, the common 'one-hot encoding' method will be used. \n",
    "C elements will have the label `[1, 0]` and F will be `[0, 1]`. \n",
    "Tuples of this form will be the ground truth labels and the model output. \n",
    "I will have to ensure that the model outputs an array of shape `[2]`.\n",
    "\"\"\"\n",
    "\n",
    "# image data, one-hot elem labels, energy\n",
    "# im_dat, el_labs, en_labs = [],[],[]\n",
    "im_dat, el_labs = [],[]\n",
    "\n",
    "energies = np.arange(50,196,5)\n",
    "\n",
    "for i in range(len(energies)): # run over all energies\n",
    "    E = energies[i]\n",
    "\n",
    "    filename = 'data/C_'+str(E)+'keV.npy'\n",
    "    print(\"Loading {}...\".format(filename))\n",
    "\n",
    "    C_dat = np.load(filename)[:,:,:]\n",
    "    # Can scale by mean and std:\n",
    "    C_dat = (C_dat-np.nanmean(C_dat,axis=0))/np.nanstd(C_dat,axis=0)\n",
    "    # C_dat = (C_dat-np.nanmean(C_dat,axis=(1,2)).reshape(-1,1,1))/np.nanstd(C_dat,axis=(1,2)).reshape(-1,1,1)\n",
    "    C_dat[C_dat!=C_dat] = 0\n",
    "\n",
    "    for n in range(np.shape(C_dat)[0]):\n",
    "        im_dat.append(C_dat[n])\n",
    "        # el_labs.append([1,0])\n",
    "        el_labs.append(i)\n",
    "\n",
    "        # en_labs.append(i)\n",
    "\n",
    "    filename = 'data/F_'+str(E)+'keV.npy'\n",
    "    print(\"Loading {}...\".format(filename))\n",
    "\n",
    "    F_dat = np.load(filename)[:,:,:]\n",
    "    F_dat = (F_dat-np.nanmean(F_dat,axis=0))/np.nanstd(F_dat,axis=0)\n",
    "    # F_dat = (F_dat-np.nanmean(F_dat,axis=(1,2)).reshape(-1,1,1))/np.nanstd(F_dat,axis=(1,2)).reshape(-1,1,1)\n",
    "    F_dat[F_dat!=F_dat] = 0\n",
    "\n",
    "    for n in range(np.shape(F_dat)[0]):\n",
    "        im_dat.append(F_dat[n])\n",
    "        # el_labs.append([0,1])\n",
    "        el_labs.append(i+len(energies)) # offset fluorines\n",
    "        # en_labs.append(i)\n",
    "        \n",
    "# multiclass classifying [carbon energies... fluorine energies]\n",
    "# Sum up probabilities of C's and F's to determine whether carbon or fluorine\n",
    "# Can then find the peak of the distribution to extract the energy?\n",
    "# Or just take most probable energy?        \n",
    "\n",
    "# C_dat = np.load('C_130keV.npy')\n",
    "\n",
    "# for n in range(np.shape(C_dat)[0]):\n",
    "#   im_dat.append(C_dat[n])\n",
    "#   el_labs.append([1,0])\n",
    "#   en_labs.append(130)\n",
    "  \n",
    "# F_dat = np.load('F_170keV.npy')\n",
    "\n",
    "# for n in range(np.shape(F_dat)[0]):\n",
    "#   im_dat.append(F_dat[n])\n",
    "#   el_labs.append([0,1])\n",
    "#   en_labs.append(170)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb242bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(im_dat, el_labs, \n",
    "                                                    test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "870ef822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240000, 1, 97, 97])\n",
      "torch.Size([240000, 1])\n"
     ]
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Training squeezing #\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "xtrain_tensor = torch.from_numpy(np.array(X_train)) #converts numpy array to torch tensor\n",
    "xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtrain_sqz.shape)\n",
    "\n",
    "\n",
    "ytrain_tensor = torch.from_numpy(np.array(y_train))\n",
    "ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "print(ytrain_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c75efd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 97, 97])\n",
      "torch.Size([60000, 1])\n"
     ]
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Testing squeezing #\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "xtest_tensor = torch.from_numpy(np.array(X_test)) #converts numpy array to torch tensor\n",
    "xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtest_sqz.shape)\n",
    "\n",
    "ytest_tensor = torch.from_numpy(np.array(y_test))\n",
    "ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "print(ytest_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cd75ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Custom dataset formatting #\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "train_data = cd.CustomDataset(xtrain_sqz.float(), ytrain_sqz.float())\n",
    "test_data = cd.CustomDataset(xtest_sqz.float(), ytest_sqz.float())\n",
    "\n",
    "\n",
    "#defining dataloader class\n",
    "train_loader = tud.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "#same as above but for test data\n",
    "test_loader = tud.DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "test_loader_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "200fe0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/240000 (0.0%)]\tLoss: 4.09567\n",
      "Train Epoch: 1 [2500/240000 (1.0%)]\tLoss: 3.74185\n",
      "Train Epoch: 1 [5000/240000 (2.0%)]\tLoss: 3.56949\n",
      "Train Epoch: 1 [7500/240000 (3.0%)]\tLoss: 3.48555\n",
      "Train Epoch: 1 [10000/240000 (4.0%)]\tLoss: 3.64515\n",
      "Train Epoch: 1 [12500/240000 (5.0%)]\tLoss: 3.45351\n",
      "Train Epoch: 1 [15000/240000 (6.0%)]\tLoss: 3.50127\n",
      "Train Epoch: 1 [17500/240000 (7.0%)]\tLoss: 3.45611\n",
      "Train Epoch: 1 [20000/240000 (8.0%)]\tLoss: 3.53782\n",
      "Train Epoch: 1 [22500/240000 (9.0%)]\tLoss: 3.43743\n",
      "Train Epoch: 1 [25000/240000 (10.0%)]\tLoss: 3.40285\n",
      "Train Epoch: 1 [27500/240000 (11.0%)]\tLoss: 3.60256\n",
      "Train Epoch: 1 [30000/240000 (12.0%)]\tLoss: 3.45678\n",
      "Train Epoch: 1 [32500/240000 (14.0%)]\tLoss: 3.43467\n",
      "Train Epoch: 1 [35000/240000 (15.0%)]\tLoss: 3.43161\n",
      "Train Epoch: 1 [37500/240000 (16.0%)]\tLoss: 3.46729\n",
      "Train Epoch: 1 [40000/240000 (17.0%)]\tLoss: 3.39112\n",
      "Train Epoch: 1 [42500/240000 (18.0%)]\tLoss: 3.42448\n",
      "Train Epoch: 1 [45000/240000 (19.0%)]\tLoss: 3.39731\n",
      "Train Epoch: 1 [47500/240000 (20.0%)]\tLoss: 3.41296\n",
      "Train Epoch: 1 [50000/240000 (21.0%)]\tLoss: 3.47992\n",
      "Train Epoch: 1 [52500/240000 (22.0%)]\tLoss: 3.40139\n",
      "Train Epoch: 1 [55000/240000 (23.0%)]\tLoss: 3.41116\n",
      "Train Epoch: 1 [57500/240000 (24.0%)]\tLoss: 3.39175\n",
      "Train Epoch: 1 [60000/240000 (25.0%)]\tLoss: 3.40839\n",
      "Train Epoch: 1 [62500/240000 (26.0%)]\tLoss: 3.42848\n",
      "Train Epoch: 1 [65000/240000 (27.0%)]\tLoss: 3.38956\n",
      "Train Epoch: 1 [67500/240000 (28.0%)]\tLoss: 3.38317\n",
      "Train Epoch: 1 [70000/240000 (29.0%)]\tLoss: 3.38937\n",
      "Train Epoch: 1 [72500/240000 (30.0%)]\tLoss: 3.35629\n",
      "Train Epoch: 1 [75000/240000 (31.0%)]\tLoss: 3.41678\n",
      "Train Epoch: 1 [77500/240000 (32.0%)]\tLoss: 3.40816\n",
      "Train Epoch: 1 [80000/240000 (33.0%)]\tLoss: 3.37628\n",
      "Train Epoch: 1 [82500/240000 (34.0%)]\tLoss: 3.39411\n",
      "Train Epoch: 1 [85000/240000 (35.0%)]\tLoss: 3.36048\n",
      "Train Epoch: 1 [87500/240000 (36.0%)]\tLoss: 3.43724\n",
      "Train Epoch: 1 [90000/240000 (38.0%)]\tLoss: 3.37629\n",
      "Train Epoch: 1 [92500/240000 (39.0%)]\tLoss: 3.40019\n",
      "Train Epoch: 1 [95000/240000 (40.0%)]\tLoss: 3.37259\n",
      "Train Epoch: 1 [97500/240000 (41.0%)]\tLoss: 3.41096\n",
      "Train Epoch: 1 [100000/240000 (42.0%)]\tLoss: 3.36307\n",
      "Train Epoch: 1 [102500/240000 (43.0%)]\tLoss: 3.36831\n",
      "Train Epoch: 1 [105000/240000 (44.0%)]\tLoss: 3.38061\n",
      "Train Epoch: 1 [107500/240000 (45.0%)]\tLoss: 3.34546\n",
      "Train Epoch: 1 [110000/240000 (46.0%)]\tLoss: 3.44176\n",
      "Train Epoch: 1 [112500/240000 (47.0%)]\tLoss: 3.33252\n",
      "Train Epoch: 1 [115000/240000 (48.0%)]\tLoss: 3.37561\n",
      "Train Epoch: 1 [117500/240000 (49.0%)]\tLoss: 3.38534\n",
      "Train Epoch: 1 [120000/240000 (50.0%)]\tLoss: 3.32815\n",
      "Train Epoch: 1 [122500/240000 (51.0%)]\tLoss: 3.33876\n",
      "Train Epoch: 1 [125000/240000 (52.0%)]\tLoss: 3.35596\n",
      "Train Epoch: 1 [127500/240000 (53.0%)]\tLoss: 3.33805\n",
      "Train Epoch: 1 [130000/240000 (54.0%)]\tLoss: 3.34818\n",
      "Train Epoch: 1 [132500/240000 (55.0%)]\tLoss: 3.46128\n",
      "Train Epoch: 1 [135000/240000 (56.0%)]\tLoss: 3.32648\n",
      "Train Epoch: 1 [137500/240000 (57.0%)]\tLoss: 3.3616\n",
      "Train Epoch: 1 [140000/240000 (58.0%)]\tLoss: 3.33299\n",
      "Train Epoch: 1 [142500/240000 (59.0%)]\tLoss: 3.34751\n",
      "Train Epoch: 1 [145000/240000 (60.0%)]\tLoss: 3.31765\n",
      "Train Epoch: 1 [147500/240000 (61.0%)]\tLoss: 3.31845\n",
      "Train Epoch: 1 [150000/240000 (62.0%)]\tLoss: 3.29985\n",
      "Train Epoch: 1 [152500/240000 (64.0%)]\tLoss: 3.33153\n",
      "Train Epoch: 1 [155000/240000 (65.0%)]\tLoss: 3.29309\n",
      "Train Epoch: 1 [157500/240000 (66.0%)]\tLoss: 3.32862\n",
      "Train Epoch: 1 [160000/240000 (67.0%)]\tLoss: 3.35102\n",
      "Train Epoch: 1 [162500/240000 (68.0%)]\tLoss: 3.30915\n",
      "Train Epoch: 1 [165000/240000 (69.0%)]\tLoss: 3.26623\n",
      "Train Epoch: 1 [167500/240000 (70.0%)]\tLoss: 3.28695\n",
      "Train Epoch: 1 [170000/240000 (71.0%)]\tLoss: 3.30051\n",
      "Train Epoch: 1 [172500/240000 (72.0%)]\tLoss: 3.27114\n",
      "Train Epoch: 1 [175000/240000 (73.0%)]\tLoss: 3.26008\n",
      "Train Epoch: 1 [177500/240000 (74.0%)]\tLoss: 3.27914\n",
      "Train Epoch: 1 [180000/240000 (75.0%)]\tLoss: 3.25508\n",
      "Train Epoch: 1 [182500/240000 (76.0%)]\tLoss: 3.24309\n",
      "Train Epoch: 1 [185000/240000 (77.0%)]\tLoss: 3.27723\n",
      "Train Epoch: 1 [187500/240000 (78.0%)]\tLoss: 3.26543\n",
      "Train Epoch: 1 [190000/240000 (79.0%)]\tLoss: 3.28009\n",
      "Train Epoch: 1 [192500/240000 (80.0%)]\tLoss: 3.25846\n",
      "Train Epoch: 1 [195000/240000 (81.0%)]\tLoss: 3.30549\n",
      "Train Epoch: 1 [197500/240000 (82.0%)]\tLoss: 3.25583\n",
      "Train Epoch: 1 [200000/240000 (83.0%)]\tLoss: 3.24666\n",
      "Train Epoch: 1 [202500/240000 (84.0%)]\tLoss: 3.26384\n",
      "Train Epoch: 1 [205000/240000 (85.0%)]\tLoss: 3.27933\n",
      "Train Epoch: 1 [207500/240000 (86.0%)]\tLoss: 3.26828\n",
      "Train Epoch: 1 [210000/240000 (88.0%)]\tLoss: 3.26801\n",
      "Train Epoch: 1 [212500/240000 (89.0%)]\tLoss: 3.2693\n",
      "Train Epoch: 1 [215000/240000 (90.0%)]\tLoss: 3.30719\n",
      "Train Epoch: 1 [217500/240000 (91.0%)]\tLoss: 3.28033\n",
      "Train Epoch: 1 [220000/240000 (92.0%)]\tLoss: 3.27278\n",
      "Train Epoch: 1 [222500/240000 (93.0%)]\tLoss: 3.24675\n",
      "Train Epoch: 1 [225000/240000 (94.0%)]\tLoss: 3.2504\n",
      "Train Epoch: 1 [227500/240000 (95.0%)]\tLoss: 3.24118\n",
      "Train Epoch: 1 [230000/240000 (96.0%)]\tLoss: 3.26869\n",
      "Train Epoch: 1 [232500/240000 (97.0%)]\tLoss: 3.23718\n",
      "Train Epoch: 1 [235000/240000 (98.0%)]\tLoss: 3.26244\n",
      "Train Epoch: 1 [237500/240000 (99.0%)]\tLoss: 3.28757\n",
      "\n",
      "Test set: Average loss: 0.065, Accuracy: 21825/60000 (36.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "model = Net.Net3_mod(2*len(energies)).to(device)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5) #stochastic gradient descent used as optimiser\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "initial_model = model\n",
    "# print(model)\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "# print(sum(numel_list), numel_list)\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "\n",
    "num_epochs = 1 #setting number of epochs for CNN\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plt.figure(figsize=(22,10))\n",
    "i = 28\n",
    "E_spec_labels = [f\"C {i} keV\" for i in energies]+[f\"F {i} keV\" for i in energies]\n",
    "E_labels = np.array(list(energies) + list(energies))\n",
    "plt.bar(\n",
    "  E_spec_labels,\n",
    "  pred[-1][i],\n",
    "  edgecolor='k',\n",
    "  lw=2\n",
    ")\n",
    "plt.xticks(rotation=\"vertical\")\n",
    "plt.ylabel(\"Weight\")\n",
    "tot_prob = np.nansum(pred[-1][i])\n",
    "C_prob = np.nansum(pred[-1][i,:int(len(energies))])\n",
    "F_prob = np.nansum(pred[-1][i,int(len(energies)):])\n",
    "C_E = np.nansum(np.arange(int(len(energies)))*pred[-1][i,:int(len(energies))]/C_prob)\n",
    "F_E = np.nansum(np.arange(int(len(energies)),2*int(len(energies)))*pred[-1][i,int(len(energies)):]/F_prob)\n",
    "pred_elem = \"C\" if C_prob > F_prob else \"F\"\n",
    "pred_E = C_E if C_prob > F_prob else F_E\n",
    "act_elem  = \"C\" if act[-1][i] < int(len(energies)) else \"F\"\n",
    "\n",
    "final_energy = np.round(E_labels[int(pred_E)] + 5 * (pred_E % int(pred_E)),1)\n",
    "\n",
    "plt.title(f\"Actual: {E_spec_labels[int(act[-1][i])]}\\nPredicted: {pred_elem} {final_energy} keV\",fontsize=20)\n",
    "\n",
    "# plt.yscale('log')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
