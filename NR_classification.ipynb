{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b105f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "import random as rd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import CustomDataset as cd\n",
    "import Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c877965",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252704a4",
   "metadata": {},
   "source": [
    "First, I'm going to try to distinguish between Carbon 130keV and Flourine 170keV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e51e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C130_dat, F170_dat = np.load('data/C_130keV.npy'), np.load('data/F_170keV.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef7124",
   "metadata": {},
   "source": [
    "Now I'm going to put them all in one file and shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b198ae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 97, 97)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C130_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14cfabc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 97, 97)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F170_dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6052e",
   "metadata": {},
   "source": [
    "For a learning algorithm, it will be necesarry to have an input (X data) containing a number of $97\\times97$ matrices for each energy. The output will be a label, either Carbon or Flourine. The model will compare its output with the actual labels and adjust accordingly to minimise loss. \n",
    "\n",
    "In order to quantify each label, the common 'one-hot encoding' method will be used. C elements will have the label `[1, 0]` and F will be `[0, 1]`. Tuples of this form will be the ground truth labels and the model output. I will have to ensure that the model outputs an array of shape `[2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dc2fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = [],[]\n",
    "\n",
    "for i in range(len(C130_dat)):\n",
    "    data.append(C130_dat[i])\n",
    "    labels.append([1,0])\n",
    "\n",
    "for i in range(len(F170_dat)):\n",
    "    data.append(F170_dat[i])\n",
    "    labels.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96639d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 97, 97)\n",
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34d5fb",
   "metadata": {},
   "source": [
    "### Defining Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb242bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "870ef822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 1, 97, 97])\n",
      "torch.Size([8000, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "xtrain_tensor = torch.from_numpy(np.array(X_train)) #converts numpy array to torch tensor\n",
    "xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtrain_sqz.shape)\n",
    "\n",
    "\n",
    "ytrain_tensor = torch.from_numpy(np.array(y_train))\n",
    "ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "print(ytrain_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c75efd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 1, 97, 97])\n",
      "torch.Size([2000, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "xtest_tensor = torch.from_numpy(np.array(X_test)) #converts numpy array to torch tensor\n",
    "xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtest_sqz.shape)\n",
    "\n",
    "ytest_tensor = torch.from_numpy(np.array(y_test))\n",
    "ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "print(ytest_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cd75ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = cd.CustomDataset(xtrain_sqz.float(), ytrain_sqz.float())\n",
    "test_data = cd.CustomDataset(xtest_sqz.float(), ytest_sqz.float())\n",
    "\n",
    "#defining dataloader class\n",
    "train_loader = tud.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "#same as above but for test data\n",
    "test_loader = tud.DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "test_loader_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f0bdc",
   "metadata": {},
   "source": [
    "### Train/Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3fb64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, device, train_loader, optimizer, epoch, batch_size = 50):\n",
    "#     model.train()\n",
    "#     #loop run over data output by loader to train model\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data) #applies model to data\n",
    "#         # print(target.shape)\n",
    "#         target = target.squeeze(1) #removes dimension from target\n",
    "#         # print(target.shape)\n",
    "#         # print(output)\n",
    "#         # print(target.shape)\n",
    "#         loss = nn.CrossEntropyLoss()(output, target[:,1].long()) #calculates cross entropy loss\n",
    "    \n",
    "# #         loss = nn.CrossEntropyLoss()(output, target.long())\n",
    "#         # print(loss)\n",
    "#         model.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step() #new step in optimisation of loss\n",
    "#         #printing output of each batch for loss observation while model is training\n",
    "#         if batch_idx % batch_size == 0:\n",
    "#             print(\n",
    "#                 'Train Epoch: {} [{}/{} ({}%)]\\tLoss: {}'.format(\n",
    "#                     epoch, batch_idx * len(data), len(train_loader.dataset), \\\n",
    "#                     round(100 * batch_idx / len(train_loader),0), round(loss.item(),5)\n",
    "#                 )\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f4202b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     #initialisting parameters and empty lists\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     probs_pred = []\n",
    "#     labels_actual = []\n",
    "#     labels_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             target=target.squeeze(1)\n",
    "#             # print(output.shape)\n",
    "#             test_loss += nn.CrossEntropyLoss()(output, target[:,1].long()).item()\n",
    "#             # print(output)\n",
    "# #             test_loss += nn.CrossEntropyLoss()(output, target.long()).item()\n",
    "\n",
    "#             # pred = output.max(1, keepdim=True)[1] #finds index of maximum in output vector as described below\n",
    "\n",
    "#             pred = output.max(1, keepdim=False)[1]#torch.round(output)\n",
    "\n",
    "#             # print(output)\n",
    "#             # pred_onehot = F.one_hot(pred).squeeze(1) #converts into one-hot predicted label \n",
    "#             # print(pred_onehot)\n",
    "#             # print(test_loss)\n",
    "#             # pred_bool = torch.eq(pred_onehot,target) #finds values where predicted equals target\n",
    "#             pred_bool = torch.eq(pred,target)\n",
    "#             # print(pred_bool)\n",
    "#             # correct += int((pred_bool.sum().item())/2) #sums number of correct values\n",
    "#             correct += pred_bool.sum().item()\n",
    "#             #adds predicted labels, targets and probabilities to lists for later use\n",
    "#             labels_actual.append(target.cpu().numpy())\n",
    "#             probs_pred.append(output.cpu().numpy())\n",
    "#             # labels_pred.append(pred_onehot.cpu().numpy())\n",
    "\n",
    "#     #printing accuracy and loss after each epoch for analysis\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     print(\n",
    "#         '\\nTest set: Average loss: {}, Accuracy: {}/{} ({}%)\\n'.format(\n",
    "#               round(test_loss,4), correct, len(test_loader.dataset), \\\n",
    "#             round(100 * correct / len(test_loader.dataset),0)\n",
    "#             )\n",
    "#           )\n",
    "  \n",
    "#     return(labels_actual,probs_pred,labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac2aa1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    #loop run over data output by loader to train model\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) #applies model to data\n",
    "        target = target.squeeze(1) #removes dimension from target\n",
    "        loss = nn.CrossEntropyLoss()(output, target) #calculates cross entropy loss\n",
    "        loss.backward()\n",
    "        optimizer.step() #new step in optimisation of loss\n",
    "        #printing output of each batch for loss observation while model is training\n",
    "        if batch_idx % batch_size == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100 * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    #initialisting parameters and empty lists\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    probs_pred = []\n",
    "    labels_actual = []\n",
    "    labels_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target=target.squeeze(1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "        \n",
    "            pred = output.max(1, keepdim=True)[1] #finds index of maximum in output vector as described below\n",
    "            pred_onehot = F.one_hot(pred).squeeze(1) #converts into one-hot predicted label \n",
    "            pred_bool = torch.eq(pred_onehot,target) #finds values where predicted equals target\n",
    "            correct += int((pred_bool.sum().item())/2) #sums number of correct values\n",
    "            #adds predicted labels, targets and probabilities to lists for later use\n",
    "            \n",
    "            labels_actual.append(target.cpu().numpy())\n",
    "            probs_pred.append(output.cpu().numpy())\n",
    "            labels_pred.append(pred_onehot.cpu().numpy())\n",
    " \n",
    "    #printing accuracy and loss after each epoch for analysis\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "    100 * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return(labels_actual,probs_pred,labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "395b0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 10, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(3,3)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(810, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80,2),\n",
    "            # nn.Softmax(dim=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 810) #flattens as described below\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c4db64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "500baf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 5, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(5, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=810, out_features=80, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=80, out_features=2, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66432, [125, 5, 1250, 10, 64800, 80, 160, 2])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputting variables from initial model\n",
    "initial_model = model\n",
    "print(model)\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "200fe0f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.745514\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.732649\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.664633\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.637937\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.703273\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.696314\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.671585\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.680831\n",
      "\n",
      "Test set: Average loss: 0.0133, Accuracy: 1191/2000 (60%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.664653\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.662531\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.625596\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.647038\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.640330\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.758945\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.622727\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.646239\n",
      "\n",
      "Test set: Average loss: 0.0130, Accuracy: 1234/2000 (62%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.665085\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.727411\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.650428\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.673306\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.605252\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.667006\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.629937\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.659263\n",
      "\n",
      "Test set: Average loss: 0.0127, Accuracy: 1249/2000 (62%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.650225\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.540306\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.621217\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.652169\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.558081\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.625466\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.575102\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.637152\n",
      "\n",
      "Test set: Average loss: 0.0127, Accuracy: 1287/2000 (64%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.585863\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.676153\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.554000\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.617972\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.605457\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.594722\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.610353\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.625154\n",
      "\n",
      "Test set: Average loss: 0.0128, Accuracy: 1281/2000 (64%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.587268\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.673923\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.620770\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.597040\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.566800\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.584540\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.612096\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.611853\n",
      "\n",
      "Test set: Average loss: 0.0125, Accuracy: 1322/2000 (66%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.612684\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.640743\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.620519\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.599149\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.606235\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.555250\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.526444\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.550696\n",
      "\n",
      "Test set: Average loss: 0.0123, Accuracy: 1334/2000 (67%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 7 #setting number of epochs for CNN\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bdff6f",
   "metadata": {},
   "source": [
    "### Different Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8f354",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('gpu')\n",
    "\n",
    "#defining CNN class, see description below\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=(6,6)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 10, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 50, kernel_size=(6,6)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(3200,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300,80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80,2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 3200) #flattens as described below\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bb6659ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "225caa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting variables from initial model\n",
    "initial_model = model\n",
    "numel_list = [p.numel() for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95ec64b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.644744\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.648438\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.626321\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.594371\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.597040\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.656058\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.661895\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.630850\n",
      "\n",
      "Test set: Average loss: 0.0130, Accuracy: 1235/2000 (62%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.616872\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.617932\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.573998\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.640386\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.665086\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.650236\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.629304\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.633588\n",
      "\n",
      "Test set: Average loss: 0.0130, Accuracy: 1258/2000 (63%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.655050\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.695628\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.638886\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.651874\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.599534\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.687259\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.648594\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.630783\n",
      "\n",
      "Test set: Average loss: 0.0129, Accuracy: 1263/2000 (63%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.609987\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.626398\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.673495\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.649877\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.655646\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.568551\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.612999\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.644433\n",
      "\n",
      "Test set: Average loss: 0.0128, Accuracy: 1263/2000 (63%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.580393\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.618520\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.642983\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.563313\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.631157\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.665868\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.594131\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.656214\n",
      "\n",
      "Test set: Average loss: 0.0128, Accuracy: 1259/2000 (63%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.591169\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.592076\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.606510\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.587341\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.630346\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.606619\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.641019\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.639067\n",
      "\n",
      "Test set: Average loss: 0.0128, Accuracy: 1275/2000 (64%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.622231\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.621550\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.653650\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.541278\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.632797\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.539835\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.623462\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.670820\n",
      "\n",
      "Test set: Average loss: 0.0128, Accuracy: 1282/2000 (64%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.569258\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.621900\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.629886\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.632614\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.565950\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.606779\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.560840\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.619338\n",
      "\n",
      "Test set: Average loss: 0.0127, Accuracy: 1285/2000 (64%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.566661\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.650227\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.581495\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.605526\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.608221\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.660930\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.600693\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.597582\n",
      "\n",
      "Test set: Average loss: 0.0127, Accuracy: 1288/2000 (64%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.579490\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.622849\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.679724\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.598970\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.631350\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.584757\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.565544\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.639620\n",
      "\n",
      "Test set: Average loss: 0.0127, Accuracy: 1271/2000 (64%)\n",
      "\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.552886\n",
      "Train Epoch: 11 [1000/8000 (12%)]\tLoss: 0.576024\n",
      "Train Epoch: 11 [2000/8000 (25%)]\tLoss: 0.589864\n",
      "Train Epoch: 11 [3000/8000 (38%)]\tLoss: 0.583727\n",
      "Train Epoch: 11 [4000/8000 (50%)]\tLoss: 0.631884\n",
      "Train Epoch: 11 [5000/8000 (62%)]\tLoss: 0.582342\n",
      "Train Epoch: 11 [6000/8000 (75%)]\tLoss: 0.599483\n",
      "Train Epoch: 11 [7000/8000 (88%)]\tLoss: 0.644714\n",
      "\n",
      "Test set: Average loss: 0.0127, Accuracy: 1299/2000 (65%)\n",
      "\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.625494\n",
      "Train Epoch: 12 [1000/8000 (12%)]\tLoss: 0.641794\n",
      "Train Epoch: 12 [2000/8000 (25%)]\tLoss: 0.593288\n",
      "Train Epoch: 12 [3000/8000 (38%)]\tLoss: 0.526459\n",
      "Train Epoch: 12 [4000/8000 (50%)]\tLoss: 0.585965\n",
      "Train Epoch: 12 [5000/8000 (62%)]\tLoss: 0.548831\n",
      "Train Epoch: 12 [6000/8000 (75%)]\tLoss: 0.635508\n",
      "Train Epoch: 12 [7000/8000 (88%)]\tLoss: 0.585970\n",
      "\n",
      "Test set: Average loss: 0.0127, Accuracy: 1282/2000 (64%)\n",
      "\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.588358\n",
      "Train Epoch: 13 [1000/8000 (12%)]\tLoss: 0.594190\n",
      "Train Epoch: 13 [2000/8000 (25%)]\tLoss: 0.612901\n",
      "Train Epoch: 13 [3000/8000 (38%)]\tLoss: 0.629554\n",
      "Train Epoch: 13 [4000/8000 (50%)]\tLoss: 0.543833\n",
      "Train Epoch: 13 [5000/8000 (62%)]\tLoss: 0.618743\n",
      "Train Epoch: 13 [6000/8000 (75%)]\tLoss: 0.615517\n",
      "Train Epoch: 13 [7000/8000 (88%)]\tLoss: 0.560812\n",
      "\n",
      "Test set: Average loss: 0.0127, Accuracy: 1291/2000 (65%)\n",
      "\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.655963\n",
      "Train Epoch: 14 [1000/8000 (12%)]\tLoss: 0.644796\n",
      "Train Epoch: 14 [2000/8000 (25%)]\tLoss: 0.588730\n",
      "Train Epoch: 14 [3000/8000 (38%)]\tLoss: 0.589097\n",
      "Train Epoch: 14 [4000/8000 (50%)]\tLoss: 0.572867\n",
      "Train Epoch: 14 [5000/8000 (62%)]\tLoss: 0.568404\n",
      "Train Epoch: 14 [6000/8000 (75%)]\tLoss: 0.617845\n",
      "Train Epoch: 14 [7000/8000 (88%)]\tLoss: 0.667274\n",
      "\n",
      "Test set: Average loss: 0.0125, Accuracy: 1318/2000 (66%)\n",
      "\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.610101\n",
      "Train Epoch: 15 [1000/8000 (12%)]\tLoss: 0.618391\n",
      "Train Epoch: 15 [2000/8000 (25%)]\tLoss: 0.592573\n",
      "Train Epoch: 15 [3000/8000 (38%)]\tLoss: 0.620521\n",
      "Train Epoch: 15 [4000/8000 (50%)]\tLoss: 0.613597\n",
      "Train Epoch: 15 [5000/8000 (62%)]\tLoss: 0.594296\n",
      "Train Epoch: 15 [6000/8000 (75%)]\tLoss: 0.557089\n",
      "Train Epoch: 15 [7000/8000 (88%)]\tLoss: 0.597237\n",
      "\n",
      "Test set: Average loss: 0.0125, Accuracy: 1306/2000 (65%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15 #setting number of epochs for CNN\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea478408",
   "metadata": {},
   "source": [
    "### Now on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5c07e417",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dat, el_labs, en_labs = [],[],[]\n",
    "for i in np.arange(50,196,5):\n",
    "    C_dat = np.load('data/C_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(C_dat)[0]):\n",
    "        im_dat.append(C_dat[n]),el_labs.append([1,0]),en_labs.append(i)\n",
    "    F_dat = np.load('data/F_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(F_dat)[0]):\n",
    "        im_dat.append(F_dat[n]),el_labs.append([0,1]),en_labs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "375d583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(im_dat, el_labs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d545af0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([240000, 1, 97, 97])\n",
      "torch.Size([240000, 1, 2])\n",
      "torch.Size([60000, 1, 97, 97])\n",
      "torch.Size([60000, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "xtrain_tensor = torch.from_numpy(np.array(X_train)) #converts numpy array to torch tensor\n",
    "xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtrain_sqz.shape)\n",
    "\n",
    "ytrain_tensor = torch.from_numpy(np.array(y_train))\n",
    "ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "print(ytrain_sqz.shape)\n",
    "\n",
    "xtest_tensor = torch.from_numpy(np.array(X_test)) #converts numpy array to torch tensor\n",
    "xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtest_sqz.shape)\n",
    "\n",
    "ytest_tensor = torch.from_numpy(np.array(y_test))\n",
    "ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "print(ytest_sqz.shape)\n",
    "\n",
    "train_data = cd.CustomDataset(xtrain_sqz.float(), ytrain_sqz.float())\n",
    "test_data = cd.CustomDataset(xtest_sqz.float(), ytest_sqz.float())\n",
    "\n",
    "#defining dataloader class\n",
    "train_loader = tud.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "#same as above but for test data\n",
    "test_loader = tud.DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "test_loader_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bd5ed258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/240000 (0%)]\tLoss: 0.665531\n",
      "Train Epoch: 1 [50000/240000 (21%)]\tLoss: 0.592201\n",
      "Train Epoch: 1 [100000/240000 (42%)]\tLoss: 0.565395\n",
      "Train Epoch: 1 [150000/240000 (62%)]\tLoss: 0.541389\n",
      "Train Epoch: 1 [200000/240000 (83%)]\tLoss: 0.544530\n",
      "\n",
      "Test set: Average loss: 0.0107, Accuracy: 45951/60000 (77%)\n",
      "\n",
      "Train Epoch: 2 [0/240000 (0%)]\tLoss: 0.538176\n",
      "Train Epoch: 2 [50000/240000 (21%)]\tLoss: 0.516556\n",
      "Train Epoch: 2 [100000/240000 (42%)]\tLoss: 0.565749\n",
      "Train Epoch: 2 [150000/240000 (62%)]\tLoss: 0.498627\n",
      "Train Epoch: 2 [200000/240000 (83%)]\tLoss: 0.497355\n",
      "\n",
      "Test set: Average loss: 0.0105, Accuracy: 46445/60000 (77%)\n",
      "\n",
      "Train Epoch: 3 [0/240000 (0%)]\tLoss: 0.474754\n",
      "Train Epoch: 3 [50000/240000 (21%)]\tLoss: 0.627133\n",
      "Train Epoch: 3 [100000/240000 (42%)]\tLoss: 0.489579\n",
      "Train Epoch: 3 [150000/240000 (62%)]\tLoss: 0.512409\n",
      "Train Epoch: 3 [200000/240000 (83%)]\tLoss: 0.475893\n",
      "\n",
      "Test set: Average loss: 0.0107, Accuracy: 45978/60000 (77%)\n",
      "\n",
      "Train Epoch: 4 [0/240000 (0%)]\tLoss: 0.464258\n",
      "Train Epoch: 4 [50000/240000 (21%)]\tLoss: 0.597205\n",
      "Train Epoch: 4 [100000/240000 (42%)]\tLoss: 0.550913\n",
      "Train Epoch: 4 [150000/240000 (62%)]\tLoss: 0.555263\n",
      "Train Epoch: 4 [200000/240000 (83%)]\tLoss: 0.546178\n",
      "\n",
      "Test set: Average loss: 0.0105, Accuracy: 46241/60000 (77%)\n",
      "\n",
      "Train Epoch: 5 [0/240000 (0%)]\tLoss: 0.587184\n",
      "Train Epoch: 5 [50000/240000 (21%)]\tLoss: 0.571292\n",
      "Train Epoch: 5 [100000/240000 (42%)]\tLoss: 0.539198\n",
      "Train Epoch: 5 [150000/240000 (62%)]\tLoss: 0.597122\n",
      "Train Epoch: 5 [200000/240000 (83%)]\tLoss: 0.583975\n",
      "\n",
      "Test set: Average loss: 0.0106, Accuracy: 46111/60000 (77%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5 #setting number of epochs for CNN\n",
    "batch_size=1000\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55a2c47",
   "metadata": {},
   "source": [
    "### On a smaller subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2b92f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(im_dat, el_labs, test_size=0.2, random_state=42)\n",
    "\n",
    "    xtrain_tensor = torch.from_numpy(np.array(X_train)) #converts numpy array to torch tensor\n",
    "    xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "    print(xtrain_sqz.shape)\n",
    "\n",
    "    ytrain_tensor = torch.from_numpy(np.array(y_train))\n",
    "    ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "    print(ytrain_sqz.shape)\n",
    "\n",
    "    xtest_tensor = torch.from_numpy(np.array(X_test)) #converts numpy array to torch tensor\n",
    "    xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "    print(xtest_sqz.shape)\n",
    "\n",
    "    ytest_tensor = torch.from_numpy(np.array(y_test))\n",
    "    ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "    print(ytest_sqz.shape)\n",
    "\n",
    "    train_data = cd.CustomDataset(xtrain_sqz.float(), ytrain_sqz.float())\n",
    "    test_data = cd.CustomDataset(xtest_sqz.float(), ytest_sqz.float())\n",
    "\n",
    "    #defining dataloader class\n",
    "    train_loader = tud.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "    train_loader_iter = iter(train_loader)\n",
    "\n",
    "    #same as above but for test data\n",
    "    test_loader = tud.DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "    test_loader_iter = iter(test_loader)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "00c6a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dat, el_labs, en_labs = [],[],[]\n",
    "for i in np.arange(60,101,5):\n",
    "    C_dat = np.load('data/C_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(C_dat)[0]):\n",
    "        im_dat.append(C_dat[n]),el_labs.append([1,0]),en_labs.append(i)\n",
    "    F_dat = np.load('data/F_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(F_dat)[0]):\n",
    "        im_dat.append(F_dat[n]),el_labs.append([0,1]),en_labs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "499c62b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([72000, 1, 97, 97])\n",
      "torch.Size([72000, 1, 2])\n",
      "torch.Size([18000, 1, 97, 97])\n",
      "torch.Size([18000, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = dataprep(im_dat,el_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ac8638aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/72000 (0%)]\tLoss: 0.420094\n",
      "Train Epoch: 1 [5000/72000 (7%)]\tLoss: 0.494878\n",
      "Train Epoch: 1 [10000/72000 (14%)]\tLoss: 0.474741\n",
      "Train Epoch: 1 [15000/72000 (21%)]\tLoss: 0.454288\n",
      "Train Epoch: 1 [20000/72000 (28%)]\tLoss: 0.445500\n",
      "Train Epoch: 1 [25000/72000 (35%)]\tLoss: 0.440105\n",
      "Train Epoch: 1 [30000/72000 (42%)]\tLoss: 0.466547\n",
      "Train Epoch: 1 [35000/72000 (49%)]\tLoss: 0.400906\n",
      "Train Epoch: 1 [40000/72000 (56%)]\tLoss: 0.454307\n",
      "Train Epoch: 1 [45000/72000 (62%)]\tLoss: 0.412948\n",
      "Train Epoch: 1 [50000/72000 (69%)]\tLoss: 0.483312\n",
      "Train Epoch: 1 [55000/72000 (76%)]\tLoss: 0.451508\n",
      "Train Epoch: 1 [60000/72000 (83%)]\tLoss: 0.476227\n",
      "Train Epoch: 1 [65000/72000 (90%)]\tLoss: 0.507730\n",
      "Train Epoch: 1 [70000/72000 (97%)]\tLoss: 0.426466\n",
      "\n",
      "Test set: Average loss: 0.0091, Accuracy: 15285/18000 (85%)\n",
      "\n",
      "Train Epoch: 2 [0/72000 (0%)]\tLoss: 0.485347\n",
      "Train Epoch: 2 [5000/72000 (7%)]\tLoss: 0.499022\n",
      "Train Epoch: 2 [10000/72000 (14%)]\tLoss: 0.574212\n",
      "Train Epoch: 2 [15000/72000 (21%)]\tLoss: 0.405212\n",
      "Train Epoch: 2 [20000/72000 (28%)]\tLoss: 0.495067\n",
      "Train Epoch: 2 [25000/72000 (35%)]\tLoss: 0.460664\n",
      "Train Epoch: 2 [30000/72000 (42%)]\tLoss: 0.427638\n",
      "Train Epoch: 2 [35000/72000 (49%)]\tLoss: 0.510762\n",
      "Train Epoch: 2 [40000/72000 (56%)]\tLoss: 0.451641\n",
      "Train Epoch: 2 [45000/72000 (62%)]\tLoss: 0.499258\n",
      "Train Epoch: 2 [50000/72000 (69%)]\tLoss: 0.558380\n",
      "Train Epoch: 2 [55000/72000 (76%)]\tLoss: 0.435476\n",
      "Train Epoch: 2 [60000/72000 (83%)]\tLoss: 0.444041\n",
      "Train Epoch: 2 [65000/72000 (90%)]\tLoss: 0.432787\n",
      "Train Epoch: 2 [70000/72000 (97%)]\tLoss: 0.455810\n",
      "\n",
      "Test set: Average loss: 0.0091, Accuracy: 15380/18000 (85%)\n",
      "\n",
      "Train Epoch: 3 [0/72000 (0%)]\tLoss: 0.449071\n",
      "Train Epoch: 3 [5000/72000 (7%)]\tLoss: 0.498018\n",
      "Train Epoch: 3 [10000/72000 (14%)]\tLoss: 0.402520\n",
      "Train Epoch: 3 [15000/72000 (21%)]\tLoss: 0.454943\n",
      "Train Epoch: 3 [20000/72000 (28%)]\tLoss: 0.443687\n",
      "Train Epoch: 3 [25000/72000 (35%)]\tLoss: 0.371224\n",
      "Train Epoch: 3 [30000/72000 (42%)]\tLoss: 0.597304\n",
      "Train Epoch: 3 [35000/72000 (49%)]\tLoss: 0.471190\n",
      "Train Epoch: 3 [40000/72000 (56%)]\tLoss: 0.468302\n",
      "Train Epoch: 3 [45000/72000 (62%)]\tLoss: 0.412379\n",
      "Train Epoch: 3 [50000/72000 (69%)]\tLoss: 0.376298\n",
      "Train Epoch: 3 [55000/72000 (76%)]\tLoss: 0.435248\n",
      "Train Epoch: 3 [60000/72000 (83%)]\tLoss: 0.451332\n",
      "Train Epoch: 3 [65000/72000 (90%)]\tLoss: 0.471141\n",
      "Train Epoch: 3 [70000/72000 (97%)]\tLoss: 0.456275\n",
      "\n",
      "Test set: Average loss: 0.0091, Accuracy: 15338/18000 (85%)\n",
      "\n",
      "Train Epoch: 4 [0/72000 (0%)]\tLoss: 0.454435\n",
      "Train Epoch: 4 [5000/72000 (7%)]\tLoss: 0.481597\n",
      "Train Epoch: 4 [10000/72000 (14%)]\tLoss: 0.381049\n",
      "Train Epoch: 4 [15000/72000 (21%)]\tLoss: 0.473658\n",
      "Train Epoch: 4 [20000/72000 (28%)]\tLoss: 0.490270\n",
      "Train Epoch: 4 [25000/72000 (35%)]\tLoss: 0.440985\n",
      "Train Epoch: 4 [30000/72000 (42%)]\tLoss: 0.475103\n",
      "Train Epoch: 4 [35000/72000 (49%)]\tLoss: 0.433621\n",
      "Train Epoch: 4 [40000/72000 (56%)]\tLoss: 0.428640\n",
      "Train Epoch: 4 [45000/72000 (62%)]\tLoss: 0.445401\n",
      "Train Epoch: 4 [50000/72000 (69%)]\tLoss: 0.428220\n",
      "Train Epoch: 4 [55000/72000 (76%)]\tLoss: 0.421682\n",
      "Train Epoch: 4 [60000/72000 (83%)]\tLoss: 0.462608\n",
      "Train Epoch: 4 [65000/72000 (90%)]\tLoss: 0.403582\n",
      "Train Epoch: 4 [70000/72000 (97%)]\tLoss: 0.403216\n",
      "\n",
      "Test set: Average loss: 0.0090, Accuracy: 15388/18000 (85%)\n",
      "\n",
      "Train Epoch: 5 [0/72000 (0%)]\tLoss: 0.446194\n",
      "Train Epoch: 5 [5000/72000 (7%)]\tLoss: 0.384409\n",
      "Train Epoch: 5 [10000/72000 (14%)]\tLoss: 0.475218\n",
      "Train Epoch: 5 [15000/72000 (21%)]\tLoss: 0.425334\n",
      "Train Epoch: 5 [20000/72000 (28%)]\tLoss: 0.459670\n",
      "Train Epoch: 5 [25000/72000 (35%)]\tLoss: 0.369614\n",
      "Train Epoch: 5 [30000/72000 (42%)]\tLoss: 0.447720\n",
      "Train Epoch: 5 [35000/72000 (49%)]\tLoss: 0.458866\n",
      "Train Epoch: 5 [40000/72000 (56%)]\tLoss: 0.417967\n",
      "Train Epoch: 5 [45000/72000 (62%)]\tLoss: 0.449995\n",
      "Train Epoch: 5 [50000/72000 (69%)]\tLoss: 0.492789\n",
      "Train Epoch: 5 [55000/72000 (76%)]\tLoss: 0.534277\n",
      "Train Epoch: 5 [60000/72000 (83%)]\tLoss: 0.424325\n",
      "Train Epoch: 5 [65000/72000 (90%)]\tLoss: 0.448598\n",
      "Train Epoch: 5 [70000/72000 (97%)]\tLoss: 0.513104\n",
      "\n",
      "Test set: Average loss: 0.0089, Accuracy: 15447/18000 (86%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5 #setting number of epochs for CNN\n",
    "batch_size=100\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b0b76d",
   "metadata": {},
   "source": [
    "### Smaller subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3b1392ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dat, el_labs, en_labs = [],[],[]\n",
    "for i in np.arange(85,101,5):\n",
    "    C_dat = np.load('data/C_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(C_dat)[0]):\n",
    "        im_dat.append(C_dat[n]),el_labs.append([1,0]),en_labs.append(i)\n",
    "    F_dat = np.load('data/F_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(F_dat)[0]):\n",
    "        im_dat.append(F_dat[n]),el_labs.append([0,1]),en_labs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "251d4d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32000, 1, 97, 97])\n",
      "torch.Size([32000, 1, 2])\n",
      "torch.Size([8000, 1, 97, 97])\n",
      "torch.Size([8000, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = dataprep(im_dat,el_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4c2b590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/32000 (0%)]\tLoss: 0.333261\n",
      "Train Epoch: 1 [5000/32000 (16%)]\tLoss: 0.353198\n",
      "Train Epoch: 1 [10000/32000 (31%)]\tLoss: 0.315129\n",
      "Train Epoch: 1 [15000/32000 (47%)]\tLoss: 0.313320\n",
      "Train Epoch: 1 [20000/32000 (62%)]\tLoss: 0.313278\n",
      "Train Epoch: 1 [25000/32000 (78%)]\tLoss: 0.313598\n",
      "Train Epoch: 1 [30000/32000 (94%)]\tLoss: 0.324402\n",
      "\n",
      "Test set: Average loss: 0.0065, Accuracy: 7923/8000 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/32000 (0%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [5000/32000 (16%)]\tLoss: 0.336176\n",
      "Train Epoch: 2 [10000/32000 (31%)]\tLoss: 0.321230\n",
      "Train Epoch: 2 [15000/32000 (47%)]\tLoss: 0.313263\n",
      "Train Epoch: 2 [20000/32000 (62%)]\tLoss: 0.314692\n",
      "Train Epoch: 2 [25000/32000 (78%)]\tLoss: 0.313262\n",
      "Train Epoch: 2 [30000/32000 (94%)]\tLoss: 0.313266\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 7956/8000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/32000 (0%)]\tLoss: 0.313350\n",
      "Train Epoch: 3 [5000/32000 (16%)]\tLoss: 0.320822\n",
      "Train Epoch: 3 [10000/32000 (31%)]\tLoss: 0.313268\n",
      "Train Epoch: 3 [15000/32000 (47%)]\tLoss: 0.314196\n",
      "Train Epoch: 3 [20000/32000 (62%)]\tLoss: 0.314171\n",
      "Train Epoch: 3 [25000/32000 (78%)]\tLoss: 0.313326\n",
      "Train Epoch: 3 [30000/32000 (94%)]\tLoss: 0.313456\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 7928/8000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/32000 (0%)]\tLoss: 0.313321\n",
      "Train Epoch: 4 [5000/32000 (16%)]\tLoss: 0.313264\n",
      "Train Epoch: 4 [10000/32000 (31%)]\tLoss: 0.313284\n",
      "Train Epoch: 4 [15000/32000 (47%)]\tLoss: 0.314391\n",
      "Train Epoch: 4 [20000/32000 (62%)]\tLoss: 0.333067\n",
      "Train Epoch: 4 [25000/32000 (78%)]\tLoss: 0.313267\n",
      "Train Epoch: 4 [30000/32000 (94%)]\tLoss: 0.313265\n",
      "\n",
      "Test set: Average loss: 0.0064, Accuracy: 7960/8000 (100%)\n",
      "\n",
      "Train Epoch: 5 [0/32000 (0%)]\tLoss: 0.313262\n",
      "Train Epoch: 5 [5000/32000 (16%)]\tLoss: 0.313262\n",
      "Train Epoch: 5 [10000/32000 (31%)]\tLoss: 0.346883\n",
      "Train Epoch: 5 [15000/32000 (47%)]\tLoss: 0.313264\n",
      "Train Epoch: 5 [20000/32000 (62%)]\tLoss: 0.331035\n",
      "Train Epoch: 5 [25000/32000 (78%)]\tLoss: 0.313272\n",
      "Train Epoch: 5 [30000/32000 (94%)]\tLoss: 0.313262\n",
      "\n",
      "Test set: Average loss: 0.0063, Accuracy: 7975/8000 (100%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5 #setting number of epochs for CNN\n",
    "batch_size=100\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d795c",
   "metadata": {},
   "source": [
    "This very high accuracy is only because the recoil tracks are much longer for Carbon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936df92a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
