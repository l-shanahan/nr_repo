{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a786de1",
   "metadata": {},
   "source": [
    "# NR Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b105f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import sys\n",
    "import random as rd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as tud\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import CustomDataset as cd\n",
    "import Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c877965",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252704a4",
   "metadata": {},
   "source": [
    "First, I'm going to try to distinguish between Carbon 130keV and Flourine 170keV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e51e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C130_dat, F170_dat = np.load('data/C_130keV.npy'), np.load('data/F_170keV.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11272ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_list = []\n",
    "for i in range(len(C130_dat)):\n",
    "    sum_list.append(np.sum(C130_dat[i]))\n",
    "    sum_list.append(np.sum(F170_dat[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f6894b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbac01c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.hist(sum_list, density=True, bins=250)  # density=False would make counts\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Total Intensity')\n",
    "plt.savefig('intensity_hist.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d99a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2068.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(C130_dat[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eef7124",
   "metadata": {},
   "source": [
    "Now I'm going to put them all in one file and shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b198ae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 97, 97)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C130_dat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14cfabc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 97, 97)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F170_dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6052e",
   "metadata": {},
   "source": [
    "For a learning algorithm, it will be necesarry to have an input (X data) containing a number of $97\\times97$ matrices for each energy. The output will be a label, either Carbon or Flourine. The model will compare its output with the actual labels and adjust accordingly to minimise loss. \n",
    "\n",
    "In order to quantify each label, the common 'one-hot encoding' method will be used. C elements will have the label `[1, 0]` and F will be `[0, 1]`. Tuples of this form will be the ground truth labels and the model output. I will have to ensure that the model outputs an array of shape `[2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dc2fc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = [],[]\n",
    "\n",
    "for i in range(len(C130_dat)):\n",
    "    data.append(C130_dat[i])\n",
    "    labels.append([1,0])\n",
    "\n",
    "for i in range(len(F170_dat)):\n",
    "    data.append(F170_dat[i])\n",
    "    labels.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f96639d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 97, 97)\n",
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(data))\n",
    "print(np.shape(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd34d5fb",
   "metadata": {},
   "source": [
    "### Defining Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb242bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "870ef822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8000, 1, 97, 97])\n",
      "torch.Size([8000, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "xtrain_tensor = torch.from_numpy(np.array(X_train)) #converts numpy array to torch tensor\n",
    "xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtrain_sqz.shape)\n",
    "\n",
    "\n",
    "ytrain_tensor = torch.from_numpy(np.array(y_train))\n",
    "ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "print(ytrain_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c75efd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 1, 97, 97])\n",
      "torch.Size([2000, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "xtest_tensor = torch.from_numpy(np.array(X_test)) #converts numpy array to torch tensor\n",
    "xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtest_sqz.shape)\n",
    "\n",
    "ytest_tensor = torch.from_numpy(np.array(y_test))\n",
    "ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "print(ytest_sqz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4cd75ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = cd.CustomDataset(xtrain_sqz.float(), ytrain_sqz.float())\n",
    "test_data = cd.CustomDataset(xtest_sqz.float(), ytest_sqz.float())\n",
    "\n",
    "#defining dataloader class\n",
    "train_loader = tud.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "#same as above but for test data\n",
    "test_loader = tud.DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "test_loader_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62f0bdc",
   "metadata": {},
   "source": [
    "### Train/Test Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3fb64ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, device, train_loader, optimizer, epoch, batch_size = 50):\n",
    "#     model.train()\n",
    "#     #loop run over data output by loader to train model\n",
    "#     for batch_idx, (data, target) in enumerate(train_loader):\n",
    "#         data, target = data.to(device), target.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         output = model(data) #applies model to data\n",
    "#         # print(target.shape)\n",
    "#         target = target.squeeze(1) #removes dimension from target\n",
    "#         # print(target.shape)\n",
    "#         # print(output)\n",
    "#         # print(target.shape)\n",
    "#         loss = nn.CrossEntropyLoss()(output, target[:,1].long()) #calculates cross entropy loss\n",
    "    \n",
    "# #         loss = nn.CrossEntropyLoss()(output, target.long())\n",
    "#         # print(loss)\n",
    "#         model.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step() #new step in optimisation of loss\n",
    "#         #printing output of each batch for loss observation while model is training\n",
    "#         if batch_idx % batch_size == 0:\n",
    "#             print(\n",
    "#                 'Train Epoch: {} [{}/{} ({}%)]\\tLoss: {}'.format(\n",
    "#                     epoch, batch_idx * len(data), len(train_loader.dataset), \\\n",
    "#                     round(100 * batch_idx / len(train_loader),0), round(loss.item(),5)\n",
    "#                 )\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f4202b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     #initialisting parameters and empty lists\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     probs_pred = []\n",
    "#     labels_actual = []\n",
    "#     labels_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             target=target.squeeze(1)\n",
    "#             # print(output.shape)\n",
    "#             test_loss += nn.CrossEntropyLoss()(output, target[:,1].long()).item()\n",
    "#             # print(output)\n",
    "# #             test_loss += nn.CrossEntropyLoss()(output, target.long()).item()\n",
    "\n",
    "#             # pred = output.max(1, keepdim=True)[1] #finds index of maximum in output vector as described below\n",
    "\n",
    "#             pred = output.max(1, keepdim=False)[1]#torch.round(output)\n",
    "\n",
    "#             # print(output)\n",
    "#             # pred_onehot = F.one_hot(pred).squeeze(1) #converts into one-hot predicted label \n",
    "#             # print(pred_onehot)\n",
    "#             # print(test_loss)\n",
    "#             # pred_bool = torch.eq(pred_onehot,target) #finds values where predicted equals target\n",
    "#             pred_bool = torch.eq(pred,target)\n",
    "#             # print(pred_bool)\n",
    "#             # correct += int((pred_bool.sum().item())/2) #sums number of correct values\n",
    "#             correct += pred_bool.sum().item()\n",
    "#             #adds predicted labels, targets and probabilities to lists for later use\n",
    "#             labels_actual.append(target.cpu().numpy())\n",
    "#             probs_pred.append(output.cpu().numpy())\n",
    "#             # labels_pred.append(pred_onehot.cpu().numpy())\n",
    "\n",
    "#     #printing accuracy and loss after each epoch for analysis\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "#     print(\n",
    "#         '\\nTest set: Average loss: {}, Accuracy: {}/{} ({}%)\\n'.format(\n",
    "#               round(test_loss,4), correct, len(test_loader.dataset), \\\n",
    "#             round(100 * correct / len(test_loader.dataset),0)\n",
    "#             )\n",
    "#           )\n",
    "  \n",
    "#     return(labels_actual,probs_pred,labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac2aa1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=20\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    #loop run over data output by loader to train model\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data) #applies model to data\n",
    "        target = target.squeeze(1) #removes dimension from target\n",
    "        loss = nn.CrossEntropyLoss()(output, target) #calculates cross entropy loss\n",
    "        loss.backward()\n",
    "        optimizer.step() #new step in optimisation of loss\n",
    "        #printing output of each batch for loss observation while model is training\n",
    "        if batch_idx % batch_size == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100 * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    #initialisting parameters and empty lists\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    probs_pred = []\n",
    "    labels_actual = []\n",
    "    labels_pred = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            target=target.squeeze(1)\n",
    "            test_loss += nn.CrossEntropyLoss()(output, target).item()\n",
    "        \n",
    "            pred = output.max(1, keepdim=True)[1] #finds index of maximum in output vector as described below\n",
    "            pred_onehot = F.one_hot(pred).squeeze(1) #converts into one-hot predicted label \n",
    "            pred_bool = torch.eq(pred_onehot,target) #finds values where predicted equals target\n",
    "            correct += int((pred_bool.sum().item())/2) #sums number of correct values\n",
    "            #adds predicted labels, targets and probabilities to lists for later use\n",
    "            \n",
    "            labels_actual.append(target.cpu().numpy())\n",
    "            probs_pred.append(output.cpu().numpy())\n",
    "            labels_pred.append(pred_onehot.cpu().numpy())\n",
    " \n",
    "    #printing accuracy and loss after each epoch for analysis\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "    100 * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    return(labels_actual,probs_pred,labels_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0069746",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(3,3)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 10, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(3,3)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(810, 80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80,2),\n",
    "            # nn.Softmax(dim=1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 810) #flattens as described below\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c3f685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "500baf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 5, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): ReLU()\n",
      "    (3): Conv2d(5, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): MaxPool2d(kernel_size=(3, 3), stride=(3, 3), padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): ReLU()\n",
      "  )\n",
      "  (fc_layers): Sequential(\n",
      "    (0): Linear(in_features=810, out_features=80, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=80, out_features=2, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66432, [125, 5, 1250, 10, 64800, 80, 160, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#outputting variables from initial model\n",
    "initial_model = model\n",
    "print(model)\n",
    "numel_list = [p.numel() for p in model.parameters()]\n",
    "sum(numel_list), numel_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "200fe0f0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.699622\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.707991\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.692986\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.694716\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.677213\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.705140\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.710313\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.720904\n",
      "\n",
      "Test set: Average loss: 0.0134, Accuracy: 1184/2000 (59%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.658818\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.659533\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.626134\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.667966\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.641279\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.637070\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.625189\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.733540\n",
      "\n",
      "Test set: Average loss: 0.0132, Accuracy: 1220/2000 (61%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.660975\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.681730\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.640374\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.622688\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.707028\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.639998\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.587146\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.576736\n",
      "\n",
      "Test set: Average loss: 0.0130, Accuracy: 1239/2000 (62%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.602108\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.584706\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.617320\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.594334\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.638394\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.657099\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.566528\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.647802\n",
      "\n",
      "Test set: Average loss: 0.0127, Accuracy: 1289/2000 (64%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.626233\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.596966\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.611176\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.643704\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.599175\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.604586\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.639460\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.654894\n",
      "\n",
      "Test set: Average loss: 0.0126, Accuracy: 1300/2000 (65%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.578059\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.651576\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.560060\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.531320\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.647310\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.661740\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.514517\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.619535\n",
      "\n",
      "Test set: Average loss: 0.0125, Accuracy: 1318/2000 (66%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.563706\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.600408\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.587548\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.534275\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.564753\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.527215\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.488366\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.644861\n",
      "\n",
      "Test set: Average loss: 0.0125, Accuracy: 1319/2000 (66%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 7 #setting number of epochs for CNN\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9345bb",
   "metadata": {},
   "source": [
    "### Different Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fb91d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "#defining CNN class, see description below\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 5, kernel_size=(6,6)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(5, 10, kernel_size=(5,5)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 50, kernel_size=(6,6)),\n",
    "            nn.MaxPool2d(kernel_size=(2,2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "            \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(3200,1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000,300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300,80),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80,2),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 3200) #flattens as described below\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ce37bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "edf2d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outputting variables from initial model\n",
    "initial_model = model\n",
    "numel_list = [p.numel() for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45074dc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/8000 (0%)]\tLoss: 0.694084\n",
      "Train Epoch: 1 [1000/8000 (12%)]\tLoss: 0.675140\n",
      "Train Epoch: 1 [2000/8000 (25%)]\tLoss: 0.695014\n",
      "Train Epoch: 1 [3000/8000 (38%)]\tLoss: 0.696487\n",
      "Train Epoch: 1 [4000/8000 (50%)]\tLoss: 0.680989\n",
      "Train Epoch: 1 [5000/8000 (62%)]\tLoss: 0.708107\n",
      "Train Epoch: 1 [6000/8000 (75%)]\tLoss: 0.671616\n",
      "Train Epoch: 1 [7000/8000 (88%)]\tLoss: 0.697973\n",
      "\n",
      "Test set: Average loss: 0.0136, Accuracy: 1138/2000 (57%)\n",
      "\n",
      "Train Epoch: 2 [0/8000 (0%)]\tLoss: 0.688464\n",
      "Train Epoch: 2 [1000/8000 (12%)]\tLoss: 0.627687\n",
      "Train Epoch: 2 [2000/8000 (25%)]\tLoss: 0.647717\n",
      "Train Epoch: 2 [3000/8000 (38%)]\tLoss: 0.677779\n",
      "Train Epoch: 2 [4000/8000 (50%)]\tLoss: 0.694466\n",
      "Train Epoch: 2 [5000/8000 (62%)]\tLoss: 0.725396\n",
      "Train Epoch: 2 [6000/8000 (75%)]\tLoss: 0.682154\n",
      "Train Epoch: 2 [7000/8000 (88%)]\tLoss: 0.691198\n",
      "\n",
      "Test set: Average loss: 0.0135, Accuracy: 1158/2000 (58%)\n",
      "\n",
      "Train Epoch: 3 [0/8000 (0%)]\tLoss: 0.648450\n",
      "Train Epoch: 3 [1000/8000 (12%)]\tLoss: 0.667011\n",
      "Train Epoch: 3 [2000/8000 (25%)]\tLoss: 0.628163\n",
      "Train Epoch: 3 [3000/8000 (38%)]\tLoss: 0.679069\n",
      "Train Epoch: 3 [4000/8000 (50%)]\tLoss: 0.690838\n",
      "Train Epoch: 3 [5000/8000 (62%)]\tLoss: 0.671030\n",
      "Train Epoch: 3 [6000/8000 (75%)]\tLoss: 0.667194\n",
      "Train Epoch: 3 [7000/8000 (88%)]\tLoss: 0.683175\n",
      "\n",
      "Test set: Average loss: 0.0134, Accuracy: 1192/2000 (60%)\n",
      "\n",
      "Train Epoch: 4 [0/8000 (0%)]\tLoss: 0.634723\n",
      "Train Epoch: 4 [1000/8000 (12%)]\tLoss: 0.670111\n",
      "Train Epoch: 4 [2000/8000 (25%)]\tLoss: 0.669834\n",
      "Train Epoch: 4 [3000/8000 (38%)]\tLoss: 0.682415\n",
      "Train Epoch: 4 [4000/8000 (50%)]\tLoss: 0.626003\n",
      "Train Epoch: 4 [5000/8000 (62%)]\tLoss: 0.585338\n",
      "Train Epoch: 4 [6000/8000 (75%)]\tLoss: 0.638628\n",
      "Train Epoch: 4 [7000/8000 (88%)]\tLoss: 0.588117\n",
      "\n",
      "Test set: Average loss: 0.0132, Accuracy: 1220/2000 (61%)\n",
      "\n",
      "Train Epoch: 5 [0/8000 (0%)]\tLoss: 0.584265\n",
      "Train Epoch: 5 [1000/8000 (12%)]\tLoss: 0.561667\n",
      "Train Epoch: 5 [2000/8000 (25%)]\tLoss: 0.607237\n",
      "Train Epoch: 5 [3000/8000 (38%)]\tLoss: 0.592955\n",
      "Train Epoch: 5 [4000/8000 (50%)]\tLoss: 0.626375\n",
      "Train Epoch: 5 [5000/8000 (62%)]\tLoss: 0.579216\n",
      "Train Epoch: 5 [6000/8000 (75%)]\tLoss: 0.623244\n",
      "Train Epoch: 5 [7000/8000 (88%)]\tLoss: 0.606072\n",
      "\n",
      "Test set: Average loss: 0.0131, Accuracy: 1194/2000 (60%)\n",
      "\n",
      "Train Epoch: 6 [0/8000 (0%)]\tLoss: 0.588178\n",
      "Train Epoch: 6 [1000/8000 (12%)]\tLoss: 0.588230\n",
      "Train Epoch: 6 [2000/8000 (25%)]\tLoss: 0.605992\n",
      "Train Epoch: 6 [3000/8000 (38%)]\tLoss: 0.552555\n",
      "Train Epoch: 6 [4000/8000 (50%)]\tLoss: 0.618963\n",
      "Train Epoch: 6 [5000/8000 (62%)]\tLoss: 0.570235\n",
      "Train Epoch: 6 [6000/8000 (75%)]\tLoss: 0.635466\n",
      "Train Epoch: 6 [7000/8000 (88%)]\tLoss: 0.662780\n",
      "\n",
      "Test set: Average loss: 0.0130, Accuracy: 1272/2000 (64%)\n",
      "\n",
      "Train Epoch: 7 [0/8000 (0%)]\tLoss: 0.563713\n",
      "Train Epoch: 7 [1000/8000 (12%)]\tLoss: 0.456163\n",
      "Train Epoch: 7 [2000/8000 (25%)]\tLoss: 0.616528\n",
      "Train Epoch: 7 [3000/8000 (38%)]\tLoss: 0.609510\n",
      "Train Epoch: 7 [4000/8000 (50%)]\tLoss: 0.597609\n",
      "Train Epoch: 7 [5000/8000 (62%)]\tLoss: 0.542850\n",
      "Train Epoch: 7 [6000/8000 (75%)]\tLoss: 0.533833\n",
      "Train Epoch: 7 [7000/8000 (88%)]\tLoss: 0.564018\n",
      "\n",
      "Test set: Average loss: 0.0131, Accuracy: 1261/2000 (63%)\n",
      "\n",
      "Train Epoch: 8 [0/8000 (0%)]\tLoss: 0.591391\n",
      "Train Epoch: 8 [1000/8000 (12%)]\tLoss: 0.529005\n",
      "Train Epoch: 8 [2000/8000 (25%)]\tLoss: 0.542105\n",
      "Train Epoch: 8 [3000/8000 (38%)]\tLoss: 0.469818\n",
      "Train Epoch: 8 [4000/8000 (50%)]\tLoss: 0.547577\n",
      "Train Epoch: 8 [5000/8000 (62%)]\tLoss: 0.505853\n",
      "Train Epoch: 8 [6000/8000 (75%)]\tLoss: 0.641616\n",
      "Train Epoch: 8 [7000/8000 (88%)]\tLoss: 0.504532\n",
      "\n",
      "Test set: Average loss: 0.0128, Accuracy: 1286/2000 (64%)\n",
      "\n",
      "Train Epoch: 9 [0/8000 (0%)]\tLoss: 0.573354\n",
      "Train Epoch: 9 [1000/8000 (12%)]\tLoss: 0.518446\n",
      "Train Epoch: 9 [2000/8000 (25%)]\tLoss: 0.532535\n",
      "Train Epoch: 9 [3000/8000 (38%)]\tLoss: 0.451321\n",
      "Train Epoch: 9 [4000/8000 (50%)]\tLoss: 0.480736\n",
      "Train Epoch: 9 [5000/8000 (62%)]\tLoss: 0.602800\n",
      "Train Epoch: 9 [6000/8000 (75%)]\tLoss: 0.544507\n",
      "Train Epoch: 9 [7000/8000 (88%)]\tLoss: 0.480016\n",
      "\n",
      "Test set: Average loss: 0.0134, Accuracy: 1230/2000 (62%)\n",
      "\n",
      "Train Epoch: 10 [0/8000 (0%)]\tLoss: 0.594432\n",
      "Train Epoch: 10 [1000/8000 (12%)]\tLoss: 0.567252\n",
      "Train Epoch: 10 [2000/8000 (25%)]\tLoss: 0.489502\n",
      "Train Epoch: 10 [3000/8000 (38%)]\tLoss: 0.465592\n",
      "Train Epoch: 10 [4000/8000 (50%)]\tLoss: 0.420957\n",
      "Train Epoch: 10 [5000/8000 (62%)]\tLoss: 0.554964\n",
      "Train Epoch: 10 [6000/8000 (75%)]\tLoss: 0.518523\n",
      "Train Epoch: 10 [7000/8000 (88%)]\tLoss: 0.554816\n",
      "\n",
      "Test set: Average loss: 0.0132, Accuracy: 1235/2000 (62%)\n",
      "\n",
      "Train Epoch: 11 [0/8000 (0%)]\tLoss: 0.487362\n",
      "Train Epoch: 11 [1000/8000 (12%)]\tLoss: 0.613074\n",
      "Train Epoch: 11 [2000/8000 (25%)]\tLoss: 0.516382\n",
      "Train Epoch: 11 [3000/8000 (38%)]\tLoss: 0.557312\n",
      "Train Epoch: 11 [4000/8000 (50%)]\tLoss: 0.477019\n",
      "Train Epoch: 11 [5000/8000 (62%)]\tLoss: 0.556942\n",
      "Train Epoch: 11 [6000/8000 (75%)]\tLoss: 0.529047\n",
      "Train Epoch: 11 [7000/8000 (88%)]\tLoss: 0.523273\n",
      "\n",
      "Test set: Average loss: 0.0132, Accuracy: 1250/2000 (62%)\n",
      "\n",
      "Train Epoch: 12 [0/8000 (0%)]\tLoss: 0.433823\n",
      "Train Epoch: 12 [1000/8000 (12%)]\tLoss: 0.413979\n",
      "Train Epoch: 12 [2000/8000 (25%)]\tLoss: 0.555861\n",
      "Train Epoch: 12 [3000/8000 (38%)]\tLoss: 0.447094\n",
      "Train Epoch: 12 [4000/8000 (50%)]\tLoss: 0.517142\n",
      "Train Epoch: 12 [5000/8000 (62%)]\tLoss: 0.544154\n",
      "Train Epoch: 12 [6000/8000 (75%)]\tLoss: 0.441705\n",
      "Train Epoch: 12 [7000/8000 (88%)]\tLoss: 0.470287\n",
      "\n",
      "Test set: Average loss: 0.0133, Accuracy: 1247/2000 (62%)\n",
      "\n",
      "Train Epoch: 13 [0/8000 (0%)]\tLoss: 0.531777\n",
      "Train Epoch: 13 [1000/8000 (12%)]\tLoss: 0.404740\n",
      "Train Epoch: 13 [2000/8000 (25%)]\tLoss: 0.424726\n",
      "Train Epoch: 13 [3000/8000 (38%)]\tLoss: 0.438598\n",
      "Train Epoch: 13 [4000/8000 (50%)]\tLoss: 0.546504\n",
      "Train Epoch: 13 [5000/8000 (62%)]\tLoss: 0.411501\n",
      "Train Epoch: 13 [6000/8000 (75%)]\tLoss: 0.447886\n",
      "Train Epoch: 13 [7000/8000 (88%)]\tLoss: 0.497179\n",
      "\n",
      "Test set: Average loss: 0.0132, Accuracy: 1266/2000 (63%)\n",
      "\n",
      "Train Epoch: 14 [0/8000 (0%)]\tLoss: 0.441258\n",
      "Train Epoch: 14 [1000/8000 (12%)]\tLoss: 0.354106\n",
      "Train Epoch: 14 [2000/8000 (25%)]\tLoss: 0.496642\n",
      "Train Epoch: 14 [3000/8000 (38%)]\tLoss: 0.386157\n",
      "Train Epoch: 14 [4000/8000 (50%)]\tLoss: 0.416991\n",
      "Train Epoch: 14 [5000/8000 (62%)]\tLoss: 0.468498\n",
      "Train Epoch: 14 [6000/8000 (75%)]\tLoss: 0.400074\n",
      "Train Epoch: 14 [7000/8000 (88%)]\tLoss: 0.425990\n",
      "\n",
      "Test set: Average loss: 0.0135, Accuracy: 1227/2000 (61%)\n",
      "\n",
      "Train Epoch: 15 [0/8000 (0%)]\tLoss: 0.440737\n",
      "Train Epoch: 15 [1000/8000 (12%)]\tLoss: 0.455609\n",
      "Train Epoch: 15 [2000/8000 (25%)]\tLoss: 0.428159\n",
      "Train Epoch: 15 [3000/8000 (38%)]\tLoss: 0.462563\n",
      "Train Epoch: 15 [4000/8000 (50%)]\tLoss: 0.368427\n",
      "Train Epoch: 15 [5000/8000 (62%)]\tLoss: 0.387019\n",
      "Train Epoch: 15 [6000/8000 (75%)]\tLoss: 0.455404\n",
      "Train Epoch: 15 [7000/8000 (88%)]\tLoss: 0.449611\n",
      "\n",
      "Test set: Average loss: 0.0133, Accuracy: 1254/2000 (63%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 15 #setting number of epochs for CNN\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff2eb7",
   "metadata": {},
   "source": [
    "### On 10,000 data points each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cde88ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "C130_10k_dat, F170_10k_dat = np.load('data/C_130keV_10000.npy'), np.load('data/F_170keV_10000.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1eae5075",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = [],[]\n",
    "\n",
    "for i in range(len(C130_dat)):\n",
    "    data.append(C130_dat[i])\n",
    "    labels.append([1,0])\n",
    "\n",
    "for i in range(len(F170_dat)):\n",
    "    data.append(F170_dat[i])\n",
    "    labels.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc227053",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(im_dat, el_labs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28e6343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([116000, 1, 97, 97])\n",
      "torch.Size([116000, 1, 2])\n",
      "torch.Size([29000, 1, 97, 97])\n",
      "torch.Size([29000, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "xtrain_tensor = torch.from_numpy(np.array(X_train)) #converts numpy array to torch tensor\n",
    "xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtrain_sqz.shape)\n",
    "\n",
    "ytrain_tensor = torch.from_numpy(np.array(y_train))\n",
    "ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "print(ytrain_sqz.shape)\n",
    "\n",
    "xtest_tensor = torch.from_numpy(np.array(X_test)) #converts numpy array to torch tensor\n",
    "xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtest_sqz.shape)\n",
    "\n",
    "ytest_tensor = torch.from_numpy(np.array(y_test))\n",
    "ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "print(ytest_sqz.shape)\n",
    "\n",
    "train_data = cd.CustomDataset(xtrain_sqz.float(), ytrain_sqz.float())\n",
    "test_data = cd.CustomDataset(xtest_sqz.float(), ytest_sqz.float())\n",
    "\n",
    "#defining dataloader class\n",
    "train_loader = tud.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "#same as above but for test data\n",
    "test_loader = tud.DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "test_loader_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a949751f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/116000 (0%)]\tLoss: 0.787554\n",
      "Train Epoch: 1 [50000/116000 (43%)]\tLoss: 0.581452\n",
      "Train Epoch: 1 [100000/116000 (86%)]\tLoss: 0.563212\n",
      "\n",
      "Test set: Average loss: 0.0108, Accuracy: 21842/29000 (75%)\n",
      "\n",
      "Train Epoch: 2 [0/116000 (0%)]\tLoss: 0.590447\n",
      "Train Epoch: 2 [50000/116000 (43%)]\tLoss: 0.473424\n",
      "Train Epoch: 2 [100000/116000 (86%)]\tLoss: 0.527991\n",
      "\n",
      "Test set: Average loss: 0.0103, Accuracy: 22546/29000 (78%)\n",
      "\n",
      "Train Epoch: 3 [0/116000 (0%)]\tLoss: 0.472085\n",
      "Train Epoch: 3 [50000/116000 (43%)]\tLoss: 0.435479\n",
      "Train Epoch: 3 [100000/116000 (86%)]\tLoss: 0.450382\n",
      "\n",
      "Test set: Average loss: 0.0102, Accuracy: 22860/29000 (79%)\n",
      "\n",
      "Train Epoch: 4 [0/116000 (0%)]\tLoss: 0.437694\n",
      "Train Epoch: 4 [50000/116000 (43%)]\tLoss: 0.550174\n",
      "Train Epoch: 4 [100000/116000 (86%)]\tLoss: 0.568644\n",
      "\n",
      "Test set: Average loss: 0.0101, Accuracy: 22921/29000 (79%)\n",
      "\n",
      "Train Epoch: 5 [0/116000 (0%)]\tLoss: 0.498107\n",
      "Train Epoch: 5 [50000/116000 (43%)]\tLoss: 0.461640\n",
      "Train Epoch: 5 [100000/116000 (86%)]\tLoss: 0.473634\n",
      "\n",
      "Test set: Average loss: 0.0101, Accuracy: 22988/29000 (79%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5 #setting number of epochs for CNN\n",
    "batch_size=1000\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc958ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c158bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f5b2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "259bbf78",
   "metadata": {},
   "source": [
    "### Now on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10c7834c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7655/582260433.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mim_dat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC_dat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mel_labs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_labs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mF_dat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/F_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'keV.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_dat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mim_dat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF_dat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mel_labs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_labs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vols/lz/MIGDAL/home/hep/lms121/vols/lz/MIGDAL/Anaconda/lib/python3.9/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_memmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0m\u001b[1;32m    441\u001b[0m                                          pickle_kwargs=pickle_kwargs)\n\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vols/lz/MIGDAL/home/hep/lms121/vols/lz/MIGDAL/Anaconda/lib/python3.9/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "im_dat, el_labs, en_labs = [],[],[]\n",
    "for i in np.arange(50,196,5):\n",
    "    C_dat = np.load('data/C_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(C_dat)[0]):\n",
    "        im_dat.append(C_dat[n]),el_labs.append([1,0]),en_labs.append(i)\n",
    "    F_dat = np.load('data/F_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(F_dat)[0]):\n",
    "        im_dat.append(F_dat[n]),el_labs.append([0,1]),en_labs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92631f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(im_dat, el_labs, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e54c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtrain_tensor = torch.from_numpy(np.array(X_train)) #converts numpy array to torch tensor\n",
    "xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtrain_sqz.shape)\n",
    "\n",
    "ytrain_tensor = torch.from_numpy(np.array(y_train))\n",
    "ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "print(ytrain_sqz.shape)\n",
    "\n",
    "xtest_tensor = torch.from_numpy(np.array(X_test)) #converts numpy array to torch tensor\n",
    "xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "print(xtest_sqz.shape)\n",
    "\n",
    "ytest_tensor = torch.from_numpy(np.array(y_test))\n",
    "ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "print(ytest_sqz.shape)\n",
    "\n",
    "train_data = cd.CustomDataset(xtrain_sqz.float(), ytrain_sqz.float())\n",
    "test_data = cd.CustomDataset(xtest_sqz.float(), ytest_sqz.float())\n",
    "\n",
    "#defining dataloader class\n",
    "train_loader = tud.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "train_loader_iter = iter(train_loader)\n",
    "\n",
    "#same as above but for test data\n",
    "test_loader = tud.DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "test_loader_iter = iter(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886fc98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 #setting number of epochs for CNN\n",
    "batch_size=1000\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05815841",
   "metadata": {},
   "source": [
    "I suspect the higher accuracies on this compared to on C130 vs F170 are due to the difference in recoil track length for the same energy towards the ends of the energy spectrum. At the ends of the spectrum it can distinguish them just by how long they are, rather than by their shape (which is what we want)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d6dc4b",
   "metadata": {},
   "source": [
    "### On a smaller subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a1b6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataprep(X,y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    xtrain_tensor = torch.from_numpy(np.array(X_train)) #converts numpy array to torch tensor\n",
    "    xtrain_sqz = xtrain_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "    print(xtrain_sqz.shape)\n",
    "\n",
    "    ytrain_tensor = torch.from_numpy(np.array(y_train))\n",
    "    ytrain_sqz = ytrain_tensor.unsqueeze(1)\n",
    "    print(ytrain_sqz.shape)\n",
    "\n",
    "    xtest_tensor = torch.from_numpy(np.array(X_test)) #converts numpy array to torch tensor\n",
    "    xtest_sqz = xtest_tensor.unsqueeze(1) #adds new dimension along axis 1\n",
    "    print(xtest_sqz.shape)\n",
    "\n",
    "    ytest_tensor = torch.from_numpy(np.array(y_test))\n",
    "    ytest_sqz = ytest_tensor.unsqueeze(1)\n",
    "    print(ytest_sqz.shape)\n",
    "\n",
    "    train_data = cd.CustomDataset(xtrain_sqz.float(), ytrain_sqz.float())\n",
    "    test_data = cd.CustomDataset(xtest_sqz.float(), ytest_sqz.float())\n",
    "\n",
    "    #defining dataloader class\n",
    "    train_loader = tud.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "    train_loader_iter = iter(train_loader)\n",
    "\n",
    "    #same as above but for test data\n",
    "    test_loader = tud.DataLoader(dataset=test_data, batch_size=50, shuffle=True)\n",
    "    test_loader_iter = iter(test_loader)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924cb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dat, el_labs, en_labs = [],[],[]\n",
    "for i in np.arange(60,101,5):\n",
    "    C_dat = np.load('data/C_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(C_dat)[0]):\n",
    "        im_dat.append(C_dat[n]),el_labs.append([1,0]),en_labs.append(i)\n",
    "    F_dat = np.load('data/F_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(F_dat)[0]):\n",
    "        im_dat.append(F_dat[n]),el_labs.append([0,1]),en_labs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = dataprep(im_dat,el_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 #setting number of epochs for CNN\n",
    "batch_size=100\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d28a2",
   "metadata": {},
   "source": [
    "### Smaller subset of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61613f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_dat, el_labs, en_labs = [],[],[]\n",
    "for i in np.arange(85,101,5):\n",
    "    C_dat = np.load('data/C_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(C_dat)[0]):\n",
    "        im_dat.append(C_dat[n]),el_labs.append([1,0]),en_labs.append(i)\n",
    "    F_dat = np.load('data/F_'+str(i)+'keV.npy')\n",
    "    for n in range(np.shape(F_dat)[0]):\n",
    "        im_dat.append(F_dat[n]),el_labs.append([0,1]),en_labs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933ad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = dataprep(im_dat,el_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df30fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 #setting number of epochs for CNN\n",
    "batch_size=100\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011703b",
   "metadata": {},
   "source": [
    "This very high accuracy is only because the recoil tracks are much longer for Carbon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee6326",
   "metadata": {},
   "source": [
    "I need more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc1d345",
   "metadata": {},
   "source": [
    "### Trying more data with C130:F170 ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab11a83",
   "metadata": {},
   "source": [
    "65:85, 130:170"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C65_dat, F85_dat = np.load('data/C_65keV.npy'), np.load('data/F_85keV.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a504d9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = [],[]\n",
    "\n",
    "for i in range(len(C130_dat)):\n",
    "    data.append(C130_dat[i])\n",
    "    labels.append([1,0])\n",
    "    data.append(C65_dat[i])\n",
    "    labels.append([1,0])\n",
    "\n",
    "for i in range(len(F170_dat)):\n",
    "    data.append(F170_dat[i])\n",
    "    labels.append([0,1])\n",
    "    data.append(F85_dat[i])\n",
    "    labels.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5e5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f938a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = dataprep(data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 #setting number of epochs for CNN\n",
    "batch_size=50\n",
    "\n",
    "#running loop training and testing over previously specified number of epochs\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    act,pred,pred_labs = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455ce25b",
   "metadata": {},
   "source": [
    "### Options for multi-class classifying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b504bb5f",
   "metadata": {},
   "source": [
    "To eliminate confusion when reading:\n",
    "- Multi-class classification is when there are more than one target labels, but each observation can only be one of those labels\n",
    "- Multi-label classification is when there are more than one target labels, and each observation can be multiple labels at the same time (eg. assigning something multiple descriptors: ' A text might be about any of religion, politics, finance or education at the same time or none of these'\n",
    "- Multi-output classification / multitask classification is when there are multiple categories of label and the observation must be one of both (eg. green and a pear, green and an apple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600f0ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b329883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca585c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
